

<!DOCTYPE html>
<html lang="en" xmlns:v-bind="http://www.w3.org/1999/xhtml">

<head>
    <!-- hexo-inject:begin --><!-- hexo-inject:end --><title>35 AN IMAGE IS WORTH 16X16 WORDS - aPaperADay</title>
<meta charset="utf-8">
<meta name="X-UA-Compatible" content="IE=edge">
<meta name="author" content="Luke Thomas">
<meta name="description" content="Vision Transformers (ViT) as simple extensions of Transformers for image data.">
<meta name="keywords" content="">

    <meta charset="utf-8">
    <meta name="X-UA-Compatible" content="IE=edge">
    <meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport">
    <meta content="telephone=no" name="format-detection">
    <meta name="renderer" content="webkit">
    <meta name="theme-color" content="#ffffff">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.1.3/dist/css/bootstrap.min.css" integrity="sha256-eSi1q2PG6J7g7ib17yAaWMcrr5GrtohYChqibrV7PBE=" crossorigin="anonymous">
<link rel="stylesheet" href="/css/journal.css?24811351">

<script src="/js/loadCSS.js"></script>
<script>
    loadCSS("https://fonts.googleapis.com/css?family=Lora|Montserrat|Fira+Mono|Material+Icons");
    (function (d) {
        var config = {
                kitId: 'dwg1tuc',
                scriptTimeout: 3000,
                async: true
            },
            h = d.documentElement, t = setTimeout(function () {
                h.className = h.className.replace(/\bwf-loading\b/g, "") + " wf-inactive";
            }, config.scriptTimeout), tk = d.createElement("script"), f = false,
            s = d.getElementsByTagName("script")[0], a;
        h.className += " wf-loading";
        tk.src = 'https://use.typekit.net/' + config.kitId + '.js';
        tk.async = true;
        tk.onload = tk.onreadystatechange = function () {
            a = this.readyState;
            if (f || a && a != "complete" && a != "loaded") return;
            f = true;
            clearTimeout(t);
            try {
                Typekit.load(config)
            } catch (e) {
            }
        };
        s.parentNode.insertBefore(tk, s)
    })(document);
</script>
<noscript>
    <link rel="stylesheet"
          href="https://fonts.googleapis.com/css?family=Lora|Montserrat|Anonymous+Pro:400|Material+Icons"/>
</noscript><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</head>
<body>
<!-- hexo-inject:begin --><!-- hexo-inject:end --><div id="top"></div>
<div id="app">
<div class="single-column-drawer-container" ref="drawer"
     v-bind:class="{ 'single-column-drawer-container-active': isDrawerOpen }">
    <div class="drawer-content">
        <div class="drawer-menu">
            <a class="a-block drawer-menu-item false" href="www.apaperaday.com">
                Home
            </a>
            
            <a class="a-block drawer-menu-item false" href="/archives">
                Archive
            </a>
            

            
            
            <a class="a-block drawer-menu-item false" href="/post.html">
                
            </a>
            
            <a class="a-block drawer-menu-item false" href="/About/index.html">
                About
            </a>
            

            
        </div>
    </div>
</div>
<transition name="fade">
    <div v-bind:class="{ 'single-column-drawer-mask': mounted }" v-if="isDrawerOpen" v-on:click="toggleDrawer"></div>
</transition>
<nav ref="navBar" class="navbar sticky-top navbar-light single-column-nav-container">
    <div ref="navBackground" class="nav-background"></div>
    <div class="container container-narrow nav-content">
        <button id="nav_dropdown_btn" class="nav-dropdown-toggle" type="button" v-on:click="toggleDrawer">
            <i class="material-icons">
                menu
            </i>
        </button>
        <a ref="navTitle" class="navbar-brand" href="/">
            aPaperADay
        </a>
    </div>
</nav>
<div class="single-column-header-container" ref="pageHead"
     v-bind:style="{ transform: 'translateZ(0px) translateY('+.3*scrollY+'px)', opacity: 1-navOpacity }">
    <a href="/">
        <div class="single-column-header-title">aPaperADay</div>
        <div class="single-column-header-subtitle"></div>
    </a>
</div>
<div ref="sideContainer" class="side-container">
    <a class="a-block nav-head false" href="/">
        <div class="nav-title">
            aPaperADay.
        </div>
        <div class="nav-subtitle">
            read a deep learning<br>paper a day
        </div>
    </a>

    <div class="nav-link-list">
        
        <a class="a-block no-tint nav-link-item false" href="/archives">
            Archive
        </a>
        

        
        
        <a class="a-block nav-link-item false" href="/post.html">
            
        </a>
        
        <a class="a-block nav-link-item false" href="/About/index.html">
            About
        </a>
        

        
    </div>

    
    <div class="nav-footer">
        Proudly published with Hexo<br>
        
        Theme <a href="https://github.com/SumiMakito/hexo-theme-journal/" target="_blank" rel="noreferrer noopener">Journal.</a> by <a href="https://mak1t0.cc/" target="_blank" rel="noreferrer noopener">Makito</a><br>
        
        &copy; 2021 <a href="www.apaperaday.com">aPaperADay</a>
    </div>
</div>
<div ref="extraContainer" class="extra-container">
    <div class="pagination">
        <a id="globalBackToTop" class="pagination-action animated-visibility" href="#top" :class="{ invisible: scrollY == 0 }">
            <i class="material-icons pagination-action-icon">
                keyboard_arrow_up
            </i>
        </a>

        
    </div>
</div>



<div ref="streamContainer" class="stream-container">
    <div class="post-list-container post-list-container-shadow">
        <div class="post">
            <div class="post-head-wrapper-text-only"
                 style="background-image: url('')">
                <div class="post-title">
                    35 AN IMAGE IS WORTH 16X16 WORDS
                    <div class="post-meta">
                        <time datetime="2021-07-16T01:23:58.000Z" itemprop="datePublished">
                            2021-07-15 18:23
                        </time>&nbsp;
                        
    
                        
                        
                        <i class="material-icons" style="">label</i>
                        
                        <a href='/tags/ViT/'>ViT</a>, 
                        
                        <a href='/tags/Transformer/'>Transformer</a>
                        
                        
                    </div>
                </div>
            </div>
    
            <div class="post-body-wrapper">
                <div class="post-body">
                    <h3 id="1-read-the-title-and-make-an-opinion-of-whats-in-the-paper-eg-the-area-the-task"><a class="markdownIt-Anchor" href="#1-read-the-title-and-make-an-opinion-of-whats-in-the-paper-eg-the-area-the-task"></a> 1. <strong>Read the title</strong> and make an opinion of what&#x2019;s in the paper (e.g., the area, the task)</h3>
<p>Year: 2020</p>
<blockquote>
<p>AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE</p>
</blockquote>
<p>There is certainly creativity employed the the designing of the title.</p>
<p>I know this paper takes the transformer architecture on images in 16x16 patches, so that seems to be the gist here. But, I wonder what &#x2018;At Scale&#x2019; means.  I probably refers to being useful on the problems we care about.</p>
<h3 id="2-read-the-abstract-well-and-form-a-hypothesis-of"><a class="markdownIt-Anchor" href="#2-read-the-abstract-well-and-form-a-hypothesis-of"></a> 2. <strong>Read the abstract well</strong> and form a hypothesis of</h3>
<ol>
<li>What&#x2019;s new in the paper?</li>
<li>Do you have a clear <em>overview</em> about what the paper is all about?</li>
</ol>
<blockquote>
<p>a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks</p>
</blockquote>
<p>This is a key contribution the paper adds to the literature.</p>
<blockquote>
<p>When pre-trained on large amounts of data and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train</p>
</blockquote>
<p>I think this is a big &#x201C;when pre-trained on large amounts of data&#x201D;. However, the trend is to release these pretrained models, making the <em>Pre-traing-&gt;Fine-Tune</em> strategy very viable.  They don&#x2019;t seem to be vying for space in the abstract though, the paper, in my opinion, assumes de-facto control of the vision Transformer landscape.</p>
<h3 id="3-look-at-the-images-and-extract-a-set-of-questions-about-what-is-not-clear-about-their-method-from-the-images-now-your-job-is-to-answer-these-questions-by-reading-the-paper"><a class="markdownIt-Anchor" href="#3-look-at-the-images-and-extract-a-set-of-questions-about-what-is-not-clear-about-their-method-from-the-images-now-your-job-is-to-answer-these-questions-by-reading-the-paper"></a> 3. <strong>Look at the images</strong> and extract a set of &#x201C;questions&#x201D; about what is not clear about their method from the images. Now your job is to answer these questions by reading the paper.</h3>
<p>So based on the figure, this is actually really easy to understand.  The ViT block is not that complicated.  What is interesting is how much work went into finding an architecture as such that could compete with CNNs.  The real question is what about this setup makes ViTs work when others weren&#x2019;t?</p>
<p><img src="fig1.png" alt="fig1"></p>
<p>Above you can see how the &#x2018;*&#x2019; token is the embedding that the MLP head is attached to in order to make classification decisions, but a great by-product of the Transformer encoder is the attention maps generally lends some interesting insights in the additional encodings.</p>
<p>A couple of Questions I have:</p>
<ol>
<li>How many layers are used?</li>
<li>Is the Linear Projection just <code>nn.Embedding</code>?</li>
</ol>
<h3 id="4-read-the-method-aiming-to-answer-your-questions-about-the-paper-focus-on-understanding-only-the-things-relevant-for-the-story-ie-to-understand-the-contribution"><a class="markdownIt-Anchor" href="#4-read-the-method-aiming-to-answer-your-questions-about-the-paper-focus-on-understanding-only-the-things-relevant-for-the-story-ie-to-understand-the-contribution"></a> 4. <strong>Read the method</strong> aiming to answer your &#x201C;questions&#x201D; about the paper. Focus on understanding only the things relevant for the story (i.e., to understand the contribution).</h3>
<p>It is simple spliting the image into 16x16 pixel patches, projecting those patches, and training a vanilla Transformer on top of that.  All quite simple.  The point out that even the positional embeddings need to be learned (in regards to 2D space).</p>
<p>Wow, and that is it.  It is quite simple to explain.</p>
<h3 id="5-read-the-experiments-to-convince-you-that-the-show-results-are-caused-by-their-claim-be-aware-that-the-experiments-highlighted-are-the-best-scenarios-and-are-fully-hyper-parameter-tuned"><a class="markdownIt-Anchor" href="#5-read-the-experiments-to-convince-you-that-the-show-results-are-caused-by-their-claim-be-aware-that-the-experiments-highlighted-are-the-best-scenarios-and-are-fully-hyper-parameter-tuned"></a> 5. <strong>Read the experiments</strong> to convince you that the show results are caused by their claim. Be aware that the experiments highlighted are the best scenarios and are fully hyper-parameter tuned.</h3>
<blockquote>
<p>When considering the computational cost of pre-training the model, ViT performs very favourably, attaining state of the art on most recognition benchmarks at a lower pre-training cost.</p>
</blockquote>
<p>They explain they are using a number of pretraining sets: Imagenet original and ImageNet021K as well as JFT (another big Image set).</p>
<p>They run 3 variants shown in Table 1.</p>
<p><img src="tab1.png" alt="tab1"></p>
<p>These results are pretty interesting.  They show the <code>Huge</code> model to perform extremely well on a number of tasks, besting other top models, while in terms of pretraining being relatively less.  Now speaking of TPUv3-core-days in the thousands is unreachable for the common man; however, seeing these results is compelling.</p>
<p><img src="tab2.png" alt="tab2"></p>
<h3 id="6-make-sure-you-answered-all-your-questions-did-the-authors-convince-you-that-their-story-has-the-effect-that-they-claim"><a class="markdownIt-Anchor" href="#6-make-sure-you-answered-all-your-questions-did-the-authors-convince-you-that-their-story-has-the-effect-that-they-claim"></a> 6. <strong>Make sure you answered all your questions.</strong> Did the authors convince you that their story has the effect that they claim?</h3>
<p>They conclude with a short aside on a <em>masked patch prediction</em> task, like MLM from BERT, and report good yet not competitive results compared to supervised pretraining.</p>
<p>I think they have good results and I think there is great future work being done on the SSL side:</p>
<blockquote>
<p>We leave exploration of contrastive pre-training (Chen et al., 2020b; He et al., 2020; Bachman et al., 2019; He &#x301;naff et al., 2020) to future work.</p>
</blockquote>

                </div>
            </div>
    
            <nav class="post-pagination">
    
    <a class="newer-posts" href="/07/20/2021/36SimCSE/">
        Previous post<br>36 SimCSE, Simple Contrastive Learning of Sentence Embeddings
    </a>
    
    <span class="page-number"></span>
    
    <a class="older-posts" href="/07/14/2021/34Combiner/">
        Next post<br>34 Combiner- Full Attention Transformer with Sparse Computation Cost
    </a>
    
</nav>

    
            


        </div>
    </div>
    <div class="single-column-footer">
    Proudly published with Hexo<br>
    
    Theme <a href="https://github.com/SumiMakito/hexo-theme-journal/" target="_blank" rel="noreferrer noopener">Journal.</a> by <a href="https://mak1t0.cc/" target="_blank" rel="noreferrer noopener">Makito</a><br>
    
    &copy; 2021 <a href="www.apaperaday.com">aPaperADay</a>
</div>
</div>

</div>
<script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"
        integrity="sha256-FgpCb/KJQlLNfOu91ta32o/NMZxltwRo8QtmkMRdAu8=" crossorigin="anonymous"></script>
<script src="https://cdn.jsdelivr.net/npm/popper.js@1.14.4/dist/umd/popper.min.js"
        integrity="sha256-EGs9T1xMHdvM1geM8jPpoo8EZ1V1VRsmcJz8OByENLA=" crossorigin="anonymous"></script>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@4.1.3/dist/js/bootstrap.min.js"
        integrity="sha256-VsEqElsCHSGmnmHXGQzvoWjWwoznFSZc6hs7ARLRacQ=" crossorigin="anonymous"></script>
<script src="https://cdn.jsdelivr.net/npm/vue@2.5.17/dist/vue.min.js"
        integrity="sha256-FtWfRI+thWlNz2sB3SJbwKx5PgMyKIVgwHCTwa3biXc=" crossorigin="anonymous"></script>
<script src="https://cdn.jsdelivr.net/npm/smooth-scroll@14.2.1/dist/smooth-scroll.polyfills.min.js"
        integrity="sha256-CI4Gq5E0io1Pv0xM3qPM+NUIOhbIBvC3GiN1Y4KhXpw=" crossorigin="anonymous"></script>
<script src="/js/journal.js?56509340"></script><!-- hexo-inject:begin --><!-- hexo-inject:end -->



</body>
</html>
